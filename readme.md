# 百度新闻爬虫与词频分析系统

## 项目简介
本项目是一套完整的百度新闻数据采集与分析工具，支持指定日期爬取百度新闻、数据去重存储、按日期查询新闻，并能对新闻标题进行分词和高频词可视化分析，适用于新闻数据监测、热点话题追踪等场景。


## 功能模块
| 模块文件 | 核心功能 | 作用说明 |
|----------|----------|----------|
| `config.py` | 配置管理 | 集中存储分词、爬虫、爬取参数、反爬策略等所有可配置项，便于灵活调整 |
| `news_crawler.py` | 新闻爬取 | 基于Selenium+BeautifulSoup实现百度新闻定向爬取，支持去重、超时控制、自动重试 |
| `news_query.py` | 新闻查询 | 读取CSV存储的新闻数据，按日期筛选查询结果并格式化输出 |
| `word_freq_plot.py` | 词频分析 | 基于jieba分词对新闻标题进行处理，提取高频词并生成可视化柱形图 |
| `utils.py` | 工具函数 | 提供CSV编码自动检测、jieba分词初始化等通用工具方法 |


## 环境准备

### 1. 依赖库安装
项目依赖以下Python库，执行以下命令一键安装：
```bash
pip install -r request.txt
```
`request.txt` 包含的依赖及其版本：
- jieba==0.42.1（中文分词）
- pandas==2.1.4（数据处理与CSV读写）
- matplotlib==3.8.2（词频可视化）
- selenium==4.15.2（浏览器自动化爬取）
- beautifulsoup4==4.12.2（HTML解析）
- tenacity==8.2.3（爬取重试机制）
- urllib3==1.26.15（网络请求支持）

### 2. Chrome浏览器与ChromeDriver配置
爬虫基于Chrome浏览器自动化实现，需提前配置环境：
1. **安装Chrome浏览器**：确保版本与ChromeDriver兼容（建议Chrome 140+，对应ChromeDriver 140版本）
2. **下载ChromeDriver**：
   - 下载地址：[ChromeDriver官方镜像](https://sites.google.com/chromium.org/driver/)
   - 解压后记录文件路径，需在`config.py`中配置
3. **配置路径**：打开`config.py`，修改以下两项为实际路径：
   ```python
   # ChromeDriver可执行文件路径
   CHROMEDRIVER_PATH = r'D:\谷歌浏览器插件\chromedriver.exe'
   # Chrome浏览器安装路径（默认路径通常无需修改，若自定义安装需调整）
   CHROME_BINARY_LOCATION = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
   ```


## 核心配置说明（config.py）
所有可调整参数均集中在`config.py`，关键配置项如下：

| 配置类别 | 配置项 | 说明 |
|----------|--------|------|
| 分词配置 | `stopwords` | 过滤无意义字词（如`['1','2','的']`），初始为空需手动添加 |
|          | `custom_words` | 自定义分词（如`['人工智能','区块链']`），确保专业术语正确分词 |
| 爬虫配置 | `CHROMEDRIVER_PATH` | ChromeDriver可执行文件路径（必配） |
|          | `CHROME_BINARY_LOCATION` | Chrome浏览器安装路径（必配） |
|          | `CSV_FILE_NAME` | 新闻数据存储的CSV文件名（默认：百度新闻.csv） |
| 爬取参数 | `TARGET_NEWS_COUNT` | 目标爬取新闻数量（默认：51条） |
|          | `TARGET_QUERY_DATE` | 目标爬取日期（格式：YYYY-MM-DD，默认：2025-10-03） |
|          | `USER_AGENT` | 浏览器标识（需与本地Chrome版本匹配，获取方式见下文） |
| 反爬配置 | `SCROLL_INTERVAL` | 随机滚动间隔（秒），默认`(2,5)`避免固定频率被识别 |
|          | `MAX_SCROLL_TIMES` | 最大滚动加载次数（默认：10次），防止无限滚动 |
|          | `TIMEOUT` | 爬取超时时间（秒，默认：60秒） |
|          | `RETRY_TIMES` | 核心请求重试次数（默认：3次），应对网络波动 |

#### 如何获取USER_AGENT？
1. 打开Chrome浏览器，在地址栏输入 `https://httpbin.org/user-agent`
2. 页面返回的`user-agent`字段值即为所需，示例：
   ```
   "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"
   ```
3. 将该值复制到`config.py`的`USER_AGENT`配置项中。


## 使用步骤

### 1. 爬取百度新闻
执行`news_crawler.py`脚本，自动爬取指定日期的百度新闻并存储到CSV文件：
```bash
python news_crawler.py
```
#### 爬取流程说明：
1. 初始化无头Chrome浏览器（不显示窗口，后台运行）
2. 访问百度新闻指定日期的搜索页面
3. 自动滚动加载页面，解析新闻标题与详情链接
4. 自动去重（基于详情链接），确保数据不重复
5. 将新闻数据（日期、标题、链接）写入CSV文件
6. 爬取完成后自动关闭浏览器，若残留进程则强制清理

#### 输出示例：
```
==================================================
百度新闻爬取程序启动（2025-10-03 10:00:00）
目标日期：2025-10-03 | 目标数量：51条
==================================================
✅ 爬取完成，共获取 51 条去重新闻（2025-10-03）

------------------------------
✅ 新CSV创建成功，保存 51 条新闻

==================================================
程序执行完成（2025-10-03 10:01:30）
总耗时：10.50秒 | CSV路径：C:\project\百度新闻.csv
==================================================
```


### 2. 查询新闻数据
执行`news_query.py`脚本，按`config.py`中配置的`TARGET_QUERY_DATE`查询新闻：
```bash
python news_query.py
```
#### 自定义查询日期：
若需查询指定日期的新闻，可在调用函数时传入日期参数，示例：
```python
# config.py的目标筛选日期（格式:YYYY-MM-DD):中修改
TARGET_QUERY_DATE = "2025-10-03"
```

#### 输出示例：
```
✅ 查询成功：2025-10-03 共 51 条新闻

--------------------------------------------------------------------------------
[1]标题：2025年10月3日国内重大新闻综述
   链接：https://www.example.com/news/123.html
--------------------------------------------------------------------------------
[2]标题：人工智能技术新突破：XX公司发布新一代大模型
   链接：https://www.example.com/news/456.html
--------------------------------------------------------------------------------
```


### 3. 词频分析与可视化
执行`word_freq_plot.py`脚本，对CSV中所有新闻标题进行分词，提取前10高频词并生成柱形图：
```bash
python word_freq_plot.py
```
#### 分析流程说明：
1. 读取CSV中的新闻标题，自动去重与空值过滤
2. 基于jieba分词对标题进行处理，过滤停用词、数字、特殊符号
3. 统计词频，提取出现次数最多的10个词
4. 生成可视化柱形图（含词频标注、网格线、样式优化）

#### 图表说明：
- 横轴：前10高频词（按频次升序排列）
- 纵轴：词频（频次数值标注在柱形顶部）
- 支持中文显示（已配置SimHei字体），图表清晰度300DPI


## 注意事项
1. **反爬策略遵守**：
   - 脚本已内置随机滚动间隔、User-Agent伪装、自动化检测屏蔽等反爬措施，请勿修改为固定频率或高频爬取，避免触发百度反爬机制导致IP受限。
2. **ChromeDriver兼容性**：
   - 若出现`ChromeDriver初始化失败`，需检查Chrome浏览器版本与ChromeDriver版本是否匹配，或路径配置是否正确。
3. **CSV文件完整性**：
   - 请勿手动修改CSV的列名（需保持`日期`、`新闻标题`、`详情页链接`），否则会导致查询与分析功能异常。
4. **停用词配置**：
   - 初始`stopwords`为空，建议根据实际需求添加无意义字词（如`['的','了','在','是']`），提升词频分析准确性。


## 常见问题排查
| 问题现象 | 可能原因 | 解决方案 |
|----------|----------|----------|
| 爬取时无新闻数据 | 1. 目标日期无新闻；2. 反爬机制拦截 | 1. 检查日期格式是否正确；2. 更换User-Agent或增加滚动间隔 |
| CSV读取失败 | 1. 文件不存在；2. 编码不支持 | 1. 先执行爬取脚本生成CSV；2. 脚本会自动检测常见编码（utf-8-sig/gbk等） |
| 词频图无中文 | 1. 未安装SimHei字体；2. 字体路径错误 | 1. 安装SimHei字体；2. 检查matplotlib字体配置是否正确 |

| 浏览器残留进程 | 爬取中断导致进程未关闭 | 脚本会自动执行`taskkill`强制清理Chrome与ChromeDriver进程，无需手动操作 |
